%! Author = air
%! Date = 10.10.2022

{}\documentclass{article}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{tikz}
\usepackage{amscd}
\usepackage[inline]{enumitem}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{indentfirst}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{epigraph}
\usepackage{icomma}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\baselinestretch}{1.0}
\renewcommand\normalsize{\sloppypar}
\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{9.1in}
\setlength{\oddsidemargin}{-0.3in}
\setlength{\textwidth}{7in}
\setlength{\parindent}{0ex}
\setlength{\parskip}{1ex}
\newtheorem{theorem}{Теорема}
\newtheorem{utv}{Утверждение}
\newtheorem{lemma}{Лемма}
% Document
\begin{document}
\textbf{Сингулярное разложение (SVD) и
Метод главных компонент (PCA)}

\textbf{Сингулярное разложение} — разложение прямоугольной матрицы, имеющее широкое применение, в силу своей наглядной
геометрической интерпретации, при решении многих прикладных задач.
Переформулировка сингулярного разложения, так называемое разложение Шмидта , имеет приложения в квантовой теории
информации, например, в квантовой запутанности.

Сингулярное разложение матрицы M позволяет вычислять сингулярные числа данной матрицы,
а также левые и правые сингулярные векторы матрицы M:

\begin{itemize}
    \item Левые сингулярные векторы матрицы M -- это собственные векторы матрицы $M M^{{*}}$.
    \item Правые сингулярные векторы матрицы M -- это собственные векторы матрицы $M^{{*}} M$.
\end{itemize}

Где $M^{*}$ -- эрмитово-сопряжённая матрица к матрице $M$, для вещественной матрицы $M^{*}=M^{T}$.

Сингулярные числа матрицы не следует путать с собственными числами той же матрицы.

Сингулярное разложение является удобным при вычислении ранга матрицы , ядра матрицы и псевдообратной матрицы .

Сингулярное разложение также используется для приближения матриц матрицами заданного ранга.

\textbf{Определение}

Пусть матрица $M$ порядка $m\times n$ состоит из элементов из поля $K$, где $K$ -- либо поле вещественных чисел,
либо поле комплексных чисел.

\textbf{Сингулярные числа и сингулярные векторы}

Неотрицательное вещественное число $\sigma$ называется сингулярным числом матрицы $M$,
когда существуют два вектора единичной длины $u\in K^{m}$ и $v\in K^{n}$ такие, что:

$Mv=\sigma u$, и $M^{*}u=\sigma v$
Такие векторы $u$ и $v$ называются, соответственно, левым сингулярным вектором и правым сингулярным вектором,
соответствующим сингулярному числу $\sigma$.

\textbf{Разложение матрицы}

Сингулярным разложением матрицы $M$ порядка $m\times n$ является разложение вида

$M=U\Sigma V^{*}$
где $\Sigma$ — диагональная матрица размера $m\times n$ с неотрицательными элементами,
у которой элементы, лежащие на главной диагонали — это сингулярные числа, а матрицы $U$ (порядка $m$) и
$V$ (порядка $n$) -- это две унитарные матрицы, состоящие из левых и правых сингулярных векторов соответственно
($V^*$ — это сопряжённо-транспонированная матрица к $V$).

\textbf{Геометрический смысл}

Пусть матрице A поставлен в соответствие линейный оператор.
Сингулярное разложение можно переформулировать в геометрических терминах.
Линейный оператор, отображающий элементы пространства $\mathbb{R}^{n}$ в себя, представим в
виде последовательно выполняемых линейных операторов вращения и растяжения.
Поэтому компоненты сингулярного разложения наглядно показывают геометрические изменения при отображении линейным
оператором $A$ множества векторов из векторного пространства в себя или в векторное пространство другой размерности.

Для более визуального представления рассмотрим сферу $S$ единичного радиуса в пространстве $\mathbb {R}^{n}$.
Линейное отображение $T$ отображает эту сферу в эллипсоид пространства $\mathbb{R} ^{m}$.
Тогда ненулевые сингулярные значения диагонали матрицы $\Sigma$ являются длинами полуосей этого эллипсоида.
В случае когда $n=m$ и все сингулярные величины различны и отличны от нуля, сингулярное разложение линейного отображения $T$
может быть легко проанализировано как последствие трех действий: рассмотрим эллипсоид $T(S)$ и его оси;
затем рассмотрим направления в $\mathbb {R}^{n}$, которые отображение $T$ переводит в эти оси.
Эти направления ортогональны.
Вначале применим изометрию ${\mathbf {v}}^{*}$, отобразив эти направления на координатные оси $\mathbb {R}^{n}$.
Вторым шагом применим эндоморфизм $\mathbf{d}$, диагонализированный вдоль координатных осей и расширяющий/сжимающий
эти направления, используя длины полуосей $T(S)$ как коэффициенты растяжения.
Тогда произведение ${\mathbf {d}}\otimes {\mathbf {v}}^{*}$ отображает единичную сферу на изометричный эллипсоид $T(S)$.
Для определения последнего шага u просто применим изометрию к этому эллипсоиду так, чтобы перевести его в $T(S)$.
Произведение ${\mathbf {u}}\otimes {\mathbf {d}}\otimes {\mathbf {v}}^{*}$ совпадает с $T$.

\textbf{Метод главных компонент} — один из основных способов уменьшить размерность данных, потеряв наименьшее
количество информации.
Вычисление главных компонент может быть сведено к вычислению сингулярного разложения матрицы данных или к вычислению
собственных векторов и собственных значений ковариационной матрицы исходных данных.

\textbf{Формальная постановка задачи}

Пусть имеется $n$ числовых признаков $f_j(x), j=1, \ldots, n$.
Объекты обучающей выборки будем отождествлять с их признаковыми описаниями:
$x_i \equiv\left(f_1\left(x_i\right), \ldots, f_n\left(x_i\right)\right), i=1, \ldots, l$. Рассмотрим матрицу $F$, строки которой соответствуют признаковым описаниям обучающих объектов:
\[
F_{l \times n}=\left(\begin{array}{ccc}
f_1\left(x_1\right) & \ldots & f_n\left(x_1\right) \\
\ldots & \ldots & \ldots \\
f_1\left(x_l\right) & \ldots & f_n\left(x_l\right)
\end{array}\right)=\left(\begin{array}{c}
x_1 \\
\ldots \\
x_l
\end{array}\right) .
\]
Обозначим через $z_i=\left(g_1\left(x_i\right), \ldots, g_m\left(x_i\right)\right)$ признаковые описания тех же объектов в новом пространстве $Z=\mathbb{R}^m$ меньшей размерности, $m<n$ :
\[
G_{l \times m}=\left(\begin{array}{ccc}
g_1\left(x_1\right) & \cdots & g_m\left(x_1\right) \\
\ldots & \ldots & \ldots \\
g_1\left(x_l\right) & \cdots & g_m\left(x_l\right)
\end{array}\right)=\left(\begin{array}{c}
z_1 \\
\ldots \\
z_l
\end{array}\right) .
\]
Потребуем, чтобы исходные признаковые описания можно было восстановить по новым описаниям с помощью некоторого линейного преобразования, определяемого матрицей $U=\left(u_{j s}\right)_{n \times m}$ :
\[
\hat{f}_j(x)=\sum_{s=1}^m g_s(x) u_{j s}, j=1, \ldots, n, x \in X,
\]
или в векторной записи: $\hat{x}=z U^T$. Восстановленное описание $\hat{x}$ не обязано в точности совпадать с исходным описанием $x$, но их отличие на объектах обучающей выборки должно быть как можно меньше при выбранной размерности $m$. Будем искать одновременно и матрицу новых признаковых описаний $G$, и матрицу линейного преобразования $U$, при которых суммарная невязка $\Delta^2(G, U)$ восстановленных описаний минимальна:
\[
\Delta^2(G, U)=\sum_{i=1}^l\left\|\hat{x}_i-x_i\right\|^2=\sum_{i=1}^l\left\|z_i U^T-x_i\right\|^2=\left\|G U^T-F\right\|^2 \rightarrow \min _{G, U},
\]
где все нормы евклидовы.
Будем предполагать, что матрицы $G$ и $U$ невырождены: $\operatorname{rank} G=\operatorname{rank} U=m$.
Иначе существовало бы представление $\bar{G} \bar{U}^T=G U^T$ с числом столбцов в матрице $\bar{G}$, меньшим $m$.
Поэтому интересны лишь случаи, когда $m \leq \operatorname{rank} F$.

\begin{theorem}
Если $m \leq \operatorname{rank} F$, то минимум $\Delta^2(G, U)$ достигается, когда столбцы матрицы $U$
есть собственные векторы $F^T F$, соответствующие $m$ максимальнып собственным значениям. При этом $G=F U$,
матрицы $U$ и $G$ ортогональны.
\end{theorem}

\begin{proof}
Запишем необходимые условия минимума:
\[
\left\{\begin{array}{l}
\frac{\partial \Delta^2}{\partial G}=\left(G U^T-F\right) U=0 \\
\frac{\partial \Delta^2}{\partial U}=G^T\left(G U^T-F\right)=0
\end{array}\right.
\]
Поскольку искомые матрицы $G$ и $U$ невырождены, отсюда следует:
\[
\left\{\begin{array}{l}
G=F U\left(U^T U\right)^{-1} \\
U=F^T G\left(G^T G\right)^{-1}
\end{array}\right.
\]
Функционал $\Delta^2(G, U)$ зависит только от произведения матриц $G U^T$, поэтому решение задачи $\Delta^2(G, U)
\rightarrow \min _{G, U}$ определено с точностью до произвольного невырожденного преобразования
$R: G U^T=(G R)\left(R^{-1} U^T\right)$. Распорядимся свободой выбора $R$ так, чтобы матрицы $U^T U$ и $G^T G$ оказались
диагональными.
Покажем, что это всегда возможно.
Пусть $\tilde{G} \tilde{U}^T-$ произвольное решение задачи.
Матрица $\tilde{U}^T \tilde{U}$ симметричная, невырожденная, положительно определенная, поэтому существует невырожденная
матрица $S_{m \times m}$ такая, что $S^{-1} \tilde{U}^T \tilde{U}\left(S^{-1}\right)^T=I_m$

Матрица $S^T \tilde{G}^T \tilde{G} S$ симметричная и невырожденная, поэтому существует ортогональная матрица
$T_{m \times m}$ такая, что $T^T\left(S^T \tilde{G}^T \tilde{G} S\right) T=\operatorname{diag}
\left(\lambda_1, \ldots, \lambda_m\right) \equiv \Lambda-$ диагональная матрица.
По определению ортогональности $T^T T=I_m$
Преобразование $R=S T$ невырождено.
Положим $G=\tilde{G} R, U^T=R^{-1} \tilde{U}^T$.
Тогда
\[
\begin{aligned}
&G^T G=T^T\left(S^T \tilde{G}^T \tilde{G} S\right) T=\Lambda \\
&U^T U=T^{-1}\left(S^{-1} \tilde{U}^T \tilde{U}\left(S^{-1}\right)^T\right)\left(T^{-1}\right)^T=\left(T^T T\right)^{-1}=I_m
\end{aligned}
\]
В силу $G U^T=\tilde{G} \tilde{U}^T$ матрицы $G$ и $U$ являются решением задачи $\Delta^2(G, U)
\rightarrow \min _{G, U}$ и удовлетворяют необходимому условию минимума. Подставим матрицы $G$ и $U$ в
\[
\begin{aligned}
&G=F U\left(U^T U\right)^{-1} \\
&U=F^T G\left(G^T G\right)^{-1}
\end{aligned}
\]
Благодаря диагональности $G^T G$ и $U^T U$ соотношения существенно упростятся:
\[
\left\{\begin{array}{l}
G=F U \\
U \Lambda=F^T G
\end{array}\right.
\]
Подставим первое соотношение во второе, получим $U \Lambda=F^T F U$.
Это означает, что столбцы матрицы $U$ обязаны быть собственными векторами матрицы $F^T F$,
а диагональные элементы $\lambda_1, \ldots, \lambda_m$ соответствующими им собственными значениями.
Аналогично, подставив второе соотношение в первое, получим $G \Lambda=F F^T G$,
то есть столбцы матрицы $G$ являются собственными векторами $F F^T$,
соответствующими тем же самым собственным значениям.
Подставляя $G$ и $U$ в функционал $\Delta^2(G, U)$, находим:
\[
\begin{aligned}
&\Delta^2(G, U)=\left\|F-G U^T\right\|^2=\operatorname{tr}\left(F^T-U G^t\right)\left(F-G U^T\right)=\operatorname{tr} F^T\left(F-G U^T\right)= \\
&\operatorname{tr} F^T F-\operatorname{tr} F^T G U^T=\|F\|^2-\operatorname{tr} U \Lambda U^T=\|F\|^2-\operatorname{tr} \Lambda=\sum_{j=1}^n \lambda_j-\sum_{j=1}^m \lambda_j-\sum_{j=m+1}^n \lambda_j
\end{aligned}
\]
где $\lambda_1, \ldots, \lambda_n$ - все собственные значения матрицы $F^T F$.
Минимум $\Delta^2$ достигается, когда $\lambda_1, \ldots, \lambda_m-$ наибольшие $m$ из $n$ собственных значений.

Собственные векторы $u_1, \ldots, u_m$, отвечающие максимальным собственным значениям, называют главными компонентами.
\end{proof}

\textbf{Связь с сингулярным разложением}

Если $m=n$, то $\Delta^2(G, U)=0$.
В этом случае представление $F=G U^T$ является точным и совпадает с сингулярным разложением:
$F=G U^T=V D U^T$, если положить $G=V D$ и $\Lambda=D^2$.
При этом матрица $V$ ортогональна: $V^T V=I_m$.

Если $m<n$, то представление $F \approx G U^T$ является приближённым.
Сингулярное разложение матрицы $G U^T$ получается из сингулярного разложения матрицы $F$ путём отбрасывания (обнуления)
$n-m$ минимальных собственных значений.
\end{document}